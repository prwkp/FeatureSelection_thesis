{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from kydavra import MultiSURFSelector\n",
    "# from sklearn.feature_selection import RFE\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from boruta import BorutaPy\n",
    "\n",
    "import shap\n",
    "\n",
    "import statistics\n",
    "\n",
    "# create the random forest model.\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "# from sklearn.model_selection import HalvingGridSearchCV\n",
    "from boruta import BorutaPy\n",
    "import random\n",
    "from sklearn.feature_selection import mutual_info_classif as MIC\n",
    "from functools import reduce\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for threshold\n",
    "def filter_arr(sample_arr, filter_index):\n",
    "    result_val = sample_arr[filter_index]\n",
    "    result_index = filter_index\n",
    "    compare_index = filter_index + 1\n",
    "    if sample_arr[0] == sample_arr[filter_index]:\n",
    "        return (0,0)\n",
    "        \n",
    "    if (len(sample_arr) -1 == filter_index):\n",
    "        compare_index = filter_index - 1\n",
    "    \n",
    "    if (sample_arr[filter_index] == sample_arr[compare_index]):\n",
    "        for index, val in enumerate(sample_arr):\n",
    "            if (val == result_val):\n",
    "                result_val = sample_arr[index - 1]\n",
    "                result_index = index - 1\n",
    "\n",
    "    return (result_val, result_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6      148             72             35        0  33.6   \n",
       "1              1       85             66             29        0  26.6   \n",
       "2              8      183             64              0        0  23.3   \n",
       "3              1       89             66             23       94  28.1   \n",
       "4              0      137             40             35      168  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10      101             76             48      180  32.9   \n",
       "764            2      122             70             27        0  36.8   \n",
       "765            5      121             72             23      112  26.2   \n",
       "766            1      126             60              0        0  30.1   \n",
       "767            1       93             70             31        0  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  class  \n",
       "0                       0.627   50      1  \n",
       "1                       0.351   31      0  \n",
       "2                       0.672   32      1  \n",
       "3                       0.167   21      0  \n",
       "4                       2.288   33      1  \n",
       "..                        ...  ...    ...  \n",
       "763                     0.171   63      0  \n",
       "764                     0.340   27      0  \n",
       "765                     0.245   30      0  \n",
       "766                     0.349   47      1  \n",
       "767                     0.315   23      0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('diabetes.csv', header=0)\n",
    "df1 = df1.rename(columns = {'Outcome':'class'})\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ = df1.drop(['class'], axis=1)\n",
    "len(X_.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mean F1 Score:  0.6060284638945773\n",
      "std F1 Score:  0.07344222364535644\n"
     ]
    }
   ],
   "source": [
    "# baseline model\n",
    "\n",
    "X = df1.drop(['class'], axis=1)\n",
    "y = df1['class']\n",
    " \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123) \n",
    "\n",
    "\n",
    "xgbc = XGBClassifier(random_state=123)\n",
    "xgbc.fit(X_train,y_train)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# #result\n",
    "# crossvalidresults(model=rfc,X=X,y=y)\n",
    "\n",
    "## results F1 and stdF1 ##\n",
    "avgF1_baselineXGBoost = cross_val_score(xgbc, X, y, scoring=\"f1\", cv = 10).mean() ##<3## \n",
    "stdF1_baselineXGBoost = cross_val_score(xgbc, X, y, scoring=\"f1\", cv = 10).std() ##<3##\n",
    "print('Mean F1 Score: ', avgF1_baselineXGBoost)\n",
    "print('std F1 Score: ', stdF1_baselineXGBoost)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source code\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delayed\n",
      "  Downloading delayed-0.11.0b1-py2.py3-none-any.whl (19 kB)\n",
      "Collecting hiredis\n",
      "  Downloading hiredis-2.2.1-cp310-cp310-win_amd64.whl (21 kB)\n",
      "Collecting redis\n",
      "  Downloading redis-4.5.1-py3-none-any.whl (238 kB)\n",
      "     -------------------------------------- 238.5/238.5 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting async-timeout>=4.0.2\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Installing collected packages: hiredis, async-timeout, redis, delayed\n",
      "Successfully installed async-timeout-4.0.2 delayed-0.11.0b1 hiredis-2.2.1 redis-4.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0\n",
      "[notice] To update, run: C:\\Users\\kornc\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\kornc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.2.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\kornc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\kornc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.23.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kornc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\kornc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0\n",
      "[notice] To update, run: C:\\Users\\kornc\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.utils._validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [45], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_selection\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_base\u001b[39;00m \u001b[39mimport\u001b[39;00m SelectorMixin\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_selection\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_base\u001b[39;00m \u001b[39mimport\u001b[39;00m _get_feature_importances\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m _validate_params\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.utils._validation'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numbers import Integral, Real\n",
    "from joblib import effective_n_jobs\n",
    "\n",
    "\n",
    "from sklearn.utils.metaestimators import available_if\n",
    "from sklearn.utils.metaestimators import _safe_split\n",
    "from sklearn.utils._param_validation import HasMethods, Interval\n",
    "from sklearn.utils._tags import _safe_tags\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils.parallel import delayed, Parallel\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import MetaEstimatorMixin\n",
    "from sklearn.base import clone\n",
    "from sklearn.base import is_classifier\n",
    "from sklearn.model_selection import check_cv\n",
    "from sklearn.model_selection._validation import _score\n",
    "from sklearn.metrics import check_scoring\n",
    "from sklearn.feature_selection._base import SelectorMixin\n",
    "from sklearn.feature_selection._base import _get_feature_importances\n",
    "# from sklearn.utils._validation import _validate_params\n",
    "\n",
    "\n",
    "\n",
    "# from ..utils.metaestimators import available_if\n",
    "# from ..utils.metaestimators import _safe_split\n",
    "# from ..utils._param_validation import HasMethods, Interval\n",
    "# from ..utils._tags import _safe_tags\n",
    "# from ..utils.validation import check_is_fitted\n",
    "# from ..utils.parallel import delayed, Parallel\n",
    "# from ..base import BaseEstimator\n",
    "# from ..base import MetaEstimatorMixin\n",
    "# from ..base import clone\n",
    "# from ..base import is_classifier\n",
    "# from ..model_selection import check_cv\n",
    "# from ..model_selection._validation import _score\n",
    "# from ..metrics import check_scoring\n",
    "# from ._base import SelectorMixin\n",
    "# from ._base import _get_feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n",
    "#          Vincent Michel <vincent.michel@inria.fr>\n",
    "#          Gilles Louppe <g.louppe@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "\"\"\"Recursive feature elimination for feature ranking\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _rfe_single_fit(rfe, estimator, X, y, train, test, scorer):\n",
    "    \"\"\"\n",
    "    Return the score for a fit across one fold.\n",
    "    \"\"\"\n",
    "    X_train, y_train = _safe_split(estimator, X, y, train)\n",
    "    X_test, y_test = _safe_split(estimator, X, y, test, train)\n",
    "    return rfe._fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        lambda estimator, features: _score(\n",
    "            estimator, X_test[:, features], y_test, scorer\n",
    "        ),\n",
    "    ).scores_\n",
    "\n",
    "\n",
    "def _estimator_has(attr):\n",
    "    \"\"\"Check if we can delegate a method to the underlying estimator.\n",
    "    First, we check the first fitted estimator if available, otherwise we\n",
    "    check the unfitted estimator.\n",
    "    \"\"\"\n",
    "    return lambda self: (\n",
    "        hasattr(self.estimator_, attr)\n",
    "        if hasattr(self, \"estimator_\")\n",
    "        else hasattr(self.estimator, attr)\n",
    "    )\n",
    "\n",
    "\n",
    "class RFE(SelectorMixin, MetaEstimatorMixin, BaseEstimator):\n",
    "    \"\"\"Feature ranking with recursive feature elimination.\n",
    "    Given an external estimator that assigns weights to features (e.g., the\n",
    "    coefficients of a linear model), the goal of recursive feature elimination\n",
    "    (RFE) is to select features by recursively considering smaller and smaller\n",
    "    sets of features. First, the estimator is trained on the initial set of\n",
    "    features and the importance of each feature is obtained either through\n",
    "    any specific attribute or callable.\n",
    "    Then, the least important features are pruned from current set of features.\n",
    "    That procedure is recursively repeated on the pruned set until the desired\n",
    "    number of features to select is eventually reached.\n",
    "    Read more in the :ref:`User Guide <rfe>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : ``Estimator`` instance\n",
    "        A supervised learning estimator with a ``fit`` method that provides\n",
    "        information about feature importance\n",
    "        (e.g. `coef_`, `feature_importances_`).\n",
    "    n_features_to_select : int or float, default=None\n",
    "        The number of features to select. If `None`, half of the features are\n",
    "        selected. If integer, the parameter is the absolute number of features\n",
    "        to select. If float between 0 and 1, it is the fraction of features to\n",
    "        select.\n",
    "        .. versionchanged:: 0.24\n",
    "           Added float values for fractions.\n",
    "    step : int or float, default=1\n",
    "        If greater than or equal to 1, then ``step`` corresponds to the\n",
    "        (integer) number of features to remove at each iteration.\n",
    "        If within (0.0, 1.0), then ``step`` corresponds to the percentage\n",
    "        (rounded down) of features to remove at each iteration.\n",
    "    verbose : int, default=0\n",
    "        Controls verbosity of output.\n",
    "    importance_getter : str or callable, default='auto'\n",
    "        If 'auto', uses the feature importance either through a `coef_`\n",
    "        or `feature_importances_` attributes of estimator.\n",
    "        Also accepts a string that specifies an attribute name/path\n",
    "        for extracting feature importance (implemented with `attrgetter`).\n",
    "        For example, give `regressor_.coef_` in case of\n",
    "        :class:`~sklearn.compose.TransformedTargetRegressor`  or\n",
    "        `named_steps.clf.feature_importances_` in case of\n",
    "        class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n",
    "        If `callable`, overrides the default feature importance getter.\n",
    "        The callable is passed with the fitted estimator and it should\n",
    "        return importance for each feature.\n",
    "        .. versionadded:: 0.24\n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : ndarray of shape (n_classes,)\n",
    "        The classes labels. Only available when `estimator` is a classifier.\n",
    "    estimator_ : ``Estimator`` instance\n",
    "        The fitted estimator used to select features.\n",
    "    n_features_ : int\n",
    "        The number of selected features.\n",
    "    n_features_in_ : int\n",
    "        Number of features seen during :term:`fit`. Only defined if the\n",
    "        underlying estimator exposes such an attribute when fit.\n",
    "        .. versionadded:: 0.24\n",
    "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "        Names of features seen during :term:`fit`. Defined only when `X`\n",
    "        has feature names that are all strings.\n",
    "        .. versionadded:: 1.0\n",
    "    ranking_ : ndarray of shape (n_features,)\n",
    "        The feature ranking, such that ``ranking_[i]`` corresponds to the\n",
    "        ranking position of the i-th feature. Selected (i.e., estimated\n",
    "        best) features are assigned rank 1.\n",
    "    support_ : ndarray of shape (n_features,)\n",
    "        The mask of selected features.\n",
    "    See Also\n",
    "    --------\n",
    "    RFECV : Recursive feature elimination with built-in cross-validated\n",
    "        selection of the best number of features.\n",
    "    SelectFromModel : Feature selection based on thresholds of importance\n",
    "        weights.\n",
    "    SequentialFeatureSelector : Sequential cross-validation based feature\n",
    "        selection. Does not rely on importance weights.\n",
    "    Notes\n",
    "    -----\n",
    "    Allows NaN/Inf in the input if the underlying estimator does as well.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n",
    "           for cancer classification using support vector machines\",\n",
    "           Mach. Learn., 46(1-3), 389--422, 2002.\n",
    "    Examples\n",
    "    --------\n",
    "    The following example shows how to retrieve the 5 most informative\n",
    "    features in the Friedman #1 dataset.\n",
    "    >>> from sklearn.datasets import make_friedman1\n",
    "    >>> from sklearn.feature_selection import RFE\n",
    "    >>> from sklearn.svm import SVR\n",
    "    >>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
    "    >>> estimator = SVR(kernel=\"linear\")\n",
    "    >>> selector = RFE(estimator, n_features_to_select=5, step=1)\n",
    "    >>> selector = selector.fit(X, y)\n",
    "    >>> selector.support_\n",
    "    array([ True,  True,  True,  True,  True, False, False, False, False,\n",
    "           False])\n",
    "    >>> selector.ranking_\n",
    "    array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
    "    \"\"\"\n",
    "\n",
    "    _parameter_constraints: dict = {\n",
    "        \"estimator\": [HasMethods([\"fit\"])],\n",
    "        \"n_features_to_select\": [\n",
    "            None,\n",
    "            Interval(Real, 0, 1, closed=\"right\"),\n",
    "            Interval(Integral, 0, None, closed=\"neither\"),\n",
    "        ],\n",
    "        \"step\": [\n",
    "            Interval(Integral, 0, None, closed=\"neither\"),\n",
    "            Interval(Real, 0, 1, closed=\"neither\"),\n",
    "        ],\n",
    "        \"verbose\": [\"verbose\"],\n",
    "        \"importance_getter\": [str, callable],\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator,\n",
    "        *,\n",
    "        n_features_to_select=None,\n",
    "        step=1,\n",
    "        verbose=0,\n",
    "        importance_getter=\"auto\",\n",
    "    ):\n",
    "        self.estimator = estimator\n",
    "        self.n_features_to_select = n_features_to_select\n",
    "        self.step = step\n",
    "        self.importance_getter = importance_getter\n",
    "        self.verbose = verbose\n",
    "\n",
    "    @property\n",
    "    def _estimator_type(self):\n",
    "        return self.estimator._estimator_type\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        \"\"\"Classes labels available when `estimator` is a classifier.\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_classes,)\n",
    "        \"\"\"\n",
    "        return self.estimator_.classes_\n",
    "\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        \"\"\"Fit the RFE model and then the underlying estimator on the selected features.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target values.\n",
    "        **fit_params : dict\n",
    "            Additional parameters passed to the `fit` method of the underlying\n",
    "            estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        # self._validate_params()\n",
    "        return self._fit(X, y, **fit_params)\n",
    "\n",
    "    def _fit(self, X, y, step_score=None, **fit_params):\n",
    "        # Parameter step_score controls the calculation of self.scores_\n",
    "        # step_score is not exposed to users\n",
    "        # and is used when implementing RFECV\n",
    "        # self.scores_ will not be calculated when calling _fit through fit\n",
    "\n",
    "        tags = self._get_tags()\n",
    "        X, y = self._validate_data(\n",
    "            X,\n",
    "            y,\n",
    "            accept_sparse=\"csc\",\n",
    "            ensure_min_features=2,\n",
    "            force_all_finite=not tags.get(\"allow_nan\", True),\n",
    "            multi_output=True,\n",
    "        )\n",
    "\n",
    "        # Initialization\n",
    "        n_features = X.shape[1]\n",
    "        if self.n_features_to_select is None:\n",
    "            n_features_to_select = n_features // 2\n",
    "            print('n_features_to_select',n_features_to_select)\n",
    "        elif isinstance(self.n_features_to_select, Integral):  # int\n",
    "            n_features_to_select = self.n_features_to_select\n",
    "        else:  # float\n",
    "            n_features_to_select = int(n_features * self.n_features_to_select)\n",
    "\n",
    "        if 0.0 < self.step < 1.0:\n",
    "            step = int(max(1, self.step * n_features))\n",
    "        else:\n",
    "            step = int(self.step)\n",
    "\n",
    "        support_ = np.ones(n_features, dtype=bool)\n",
    "        ranking_ = np.ones(n_features, dtype=int)\n",
    "\n",
    "        if step_score:\n",
    "            self.scores_ = []\n",
    "\n",
    "        # Elimination\n",
    "        print('Elimination')\n",
    "        while np.sum(support_) > n_features_to_select:\n",
    "            # Remaining features\n",
    "            features = np.arange(n_features)[support_]\n",
    "            print('Remaining features: while np.sum(support_) > n_features_to_select',features)\n",
    "\n",
    "            # Rank the remaining features\n",
    "            estimator = clone(self.estimator)\n",
    "            if self.verbose > 0:\n",
    "                print('Rank the remaining features')\n",
    "                print(\"Fitting estimator with %d features.\" % np.sum(support_))\n",
    "\n",
    "            estimator.fit(X[:, features], y, **fit_params)\n",
    "\n",
    "            # Get importance and rank them\n",
    "            importances = _get_feature_importances(\n",
    "                estimator,\n",
    "                self.importance_getter,\n",
    "                transform_func=\"square\",\n",
    "            )\n",
    "            ranks = np.argsort(importances)\n",
    "            print('Get importance and rank them: ranks',ranks)\n",
    "\n",
    "            # for sparse case ranks is matrix\n",
    "            ranks = np.ravel(ranks)\n",
    "\n",
    "            # Eliminate the worse features\n",
    "            threshold = min(step, np.sum(support_) - n_features_to_select)\n",
    "            print('threshold = min(step, np.sum(support_) - n_features_to_select', threshold)\n",
    "\n",
    "            # Compute step score on the previous selection iteration\n",
    "            # because 'estimator' must use features\n",
    "            # that have not been eliminated yet\n",
    "            if step_score:\n",
    "                self.scores_.append(step_score(estimator, features))\n",
    "            support_[features[ranks][:threshold]] = False\n",
    "            ranking_[np.logical_not(support_)] += 1\n",
    "        print('ranking_',ranking_)\n",
    "\n",
    "        # Set final attributes\n",
    "        features = np.arange(n_features)[support_]\n",
    "        self.estimator_ = clone(self.estimator)\n",
    "        self.estimator_.fit(X[:, features], y, **fit_params)\n",
    "        print('Set final attributes',features)\n",
    "\n",
    "        # Compute step score when only n_features_to_select features left\n",
    "        if step_score:\n",
    "            self.scores_.append(step_score(self.estimator_, features))\n",
    "        self.n_features_ = support_.sum()\n",
    "        self.support_ = support_\n",
    "        self.ranking_ = ranking_\n",
    "\n",
    "        return self\n",
    "\n",
    "    @available_if(_estimator_has(\"predict\"))\n",
    "    def predict(self, X):\n",
    "        \"\"\"Reduce X to the selected features and predict using the estimator.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape [n_samples, n_features]\n",
    "            The input samples.\n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape [n_samples]\n",
    "            The predicted target values.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        return self.estimator_.predict(self.transform(X))\n",
    "\n",
    "    @available_if(_estimator_has(\"score\"))\n",
    "    def score(self, X, y, **fit_params):\n",
    "        \"\"\"Reduce X to the selected features and return the score of the estimator.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape [n_samples, n_features]\n",
    "            The input samples.\n",
    "        y : array of shape [n_samples]\n",
    "            The target values.\n",
    "        **fit_params : dict\n",
    "            Parameters to pass to the `score` method of the underlying\n",
    "            estimator.\n",
    "            .. versionadded:: 1.0\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            Score of the underlying base estimator computed with the selected\n",
    "            features returned by `rfe.transform(X)` and `y`.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        return self.estimator_.score(self.transform(X), y, **fit_params)\n",
    "\n",
    "    def _get_support_mask(self):\n",
    "        check_is_fitted(self)\n",
    "        return self.support_\n",
    "\n",
    "    @available_if(_estimator_has(\"decision_function\"))\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Compute the decision function of ``X``.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like or sparse matrix} of shape (n_samples, n_features)\n",
    "            The input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
    "            to a sparse ``csr_matrix``.\n",
    "        Returns\n",
    "        -------\n",
    "        score : array, shape = [n_samples, n_classes] or [n_samples]\n",
    "            The decision function of the input samples. The order of the\n",
    "            classes corresponds to that in the attribute :term:`classes_`.\n",
    "            Regression and binary classification produce an array of shape\n",
    "            [n_samples].\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        return self.estimator_.decision_function(self.transform(X))\n",
    "\n",
    "    @available_if(_estimator_has(\"predict_proba\"))\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like or sparse matrix} of shape (n_samples, n_features)\n",
    "            The input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
    "            to a sparse ``csr_matrix``.\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape (n_samples, n_classes)\n",
    "            The class probabilities of the input samples. The order of the\n",
    "            classes corresponds to that in the attribute :term:`classes_`.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        return self.estimator_.predict_proba(self.transform(X))\n",
    "\n",
    "    @available_if(_estimator_has(\"predict_log_proba\"))\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"Predict class log-probabilities for X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape [n_samples, n_features]\n",
    "            The input samples.\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape (n_samples, n_classes)\n",
    "            The class log-probabilities of the input samples. The order of the\n",
    "            classes corresponds to that in the attribute :term:`classes_`.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        return self.estimator_.predict_log_proba(self.transform(X))\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\n",
    "            \"poor_score\": True,\n",
    "            \"allow_nan\": _safe_tags(self.estimator, key=\"allow_nan\"),\n",
    "            \"requires_y\": True,\n",
    "        }\n",
    "\n",
    "\n",
    "class RFECV(RFE):\n",
    "    \"\"\"Recursive feature elimination with cross-validation to select features.\n",
    "    See glossary entry for :term:`cross-validation estimator`.\n",
    "    Read more in the :ref:`User Guide <rfe>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : ``Estimator`` instance\n",
    "        A supervised learning estimator with a ``fit`` method that provides\n",
    "        information about feature importance either through a ``coef_``\n",
    "        attribute or through a ``feature_importances_`` attribute.\n",
    "    step : int or float, default=1\n",
    "        If greater than or equal to 1, then ``step`` corresponds to the\n",
    "        (integer) number of features to remove at each iteration.\n",
    "        If within (0.0, 1.0), then ``step`` corresponds to the percentage\n",
    "        (rounded down) of features to remove at each iteration.\n",
    "        Note that the last iteration may remove fewer than ``step`` features in\n",
    "        order to reach ``min_features_to_select``.\n",
    "    min_features_to_select : int, default=1\n",
    "        The minimum number of features to be selected. This number of features\n",
    "        will always be scored, even if the difference between the original\n",
    "        feature count and ``min_features_to_select`` isn't divisible by\n",
    "        ``step``.\n",
    "        .. versionadded:: 0.20\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "        - None, to use the default 5-fold cross-validation,\n",
    "        - integer, to specify the number of folds.\n",
    "        - :term:`CV splitter`,\n",
    "        - An iterable yielding (train, test) splits as arrays of indices.\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`~sklearn.model_selection.StratifiedKFold` is used. If the\n",
    "        estimator is a classifier or if ``y`` is neither binary nor multiclass,\n",
    "        :class:`~sklearn.model_selection.KFold` is used.\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validation strategies that can be used here.\n",
    "        .. versionchanged:: 0.22\n",
    "            ``cv`` default value of None changed from 3-fold to 5-fold.\n",
    "    scoring : str, callable or None, default=None\n",
    "        A string (see model evaluation documentation) or\n",
    "        a scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "    verbose : int, default=0\n",
    "        Controls verbosity of output.\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of cores to run in parallel while fitting across folds.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "        .. versionadded:: 0.18\n",
    "    importance_getter : str or callable, default='auto'\n",
    "        If 'auto', uses the feature importance either through a `coef_`\n",
    "        or `feature_importances_` attributes of estimator.\n",
    "        Also accepts a string that specifies an attribute name/path\n",
    "        for extracting feature importance.\n",
    "        For example, give `regressor_.coef_` in case of\n",
    "        :class:`~sklearn.compose.TransformedTargetRegressor`  or\n",
    "        `named_steps.clf.feature_importances_` in case of\n",
    "        :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n",
    "        If `callable`, overrides the default feature importance getter.\n",
    "        The callable is passed with the fitted estimator and it should\n",
    "        return importance for each feature.\n",
    "        .. versionadded:: 0.24\n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : ndarray of shape (n_classes,)\n",
    "        The classes labels. Only available when `estimator` is a classifier.\n",
    "    estimator_ : ``Estimator`` instance\n",
    "        The fitted estimator used to select features.\n",
    "    cv_results_ : dict of ndarrays\n",
    "        A dict with keys:\n",
    "        split(k)_test_score : ndarray of shape (n_subsets_of_features,)\n",
    "            The cross-validation scores across (k)th fold.\n",
    "        mean_test_score : ndarray of shape (n_subsets_of_features,)\n",
    "            Mean of scores over the folds.\n",
    "        std_test_score : ndarray of shape (n_subsets_of_features,)\n",
    "            Standard deviation of scores over the folds.\n",
    "        .. versionadded:: 1.0\n",
    "    n_features_ : int\n",
    "        The number of selected features with cross-validation.\n",
    "    n_features_in_ : int\n",
    "        Number of features seen during :term:`fit`. Only defined if the\n",
    "        underlying estimator exposes such an attribute when fit.\n",
    "        .. versionadded:: 0.24\n",
    "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "        Names of features seen during :term:`fit`. Defined only when `X`\n",
    "        has feature names that are all strings.\n",
    "        .. versionadded:: 1.0\n",
    "    ranking_ : narray of shape (n_features,)\n",
    "        The feature ranking, such that `ranking_[i]`\n",
    "        corresponds to the ranking\n",
    "        position of the i-th feature.\n",
    "        Selected (i.e., estimated best)\n",
    "        features are assigned rank 1.\n",
    "    support_ : ndarray of shape (n_features,)\n",
    "        The mask of selected features.\n",
    "    See Also\n",
    "    --------\n",
    "    RFE : Recursive feature elimination.\n",
    "    Notes\n",
    "    -----\n",
    "    The size of all values in ``cv_results_`` is equal to\n",
    "    ``ceil((n_features - min_features_to_select) / step) + 1``,\n",
    "    where step is the number of features removed at each iteration.\n",
    "    Allows NaN/Inf in the input if the underlying estimator does as well.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n",
    "           for cancer classification using support vector machines\",\n",
    "           Mach. Learn., 46(1-3), 389--422, 2002.\n",
    "    Examples\n",
    "    --------\n",
    "    The following example shows how to retrieve the a-priori not known 5\n",
    "    informative features in the Friedman #1 dataset.\n",
    "    >>> from sklearn.datasets import make_friedman1\n",
    "    >>> from sklearn.feature_selection import RFECV\n",
    "    >>> from sklearn.svm import SVR\n",
    "    >>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
    "    >>> estimator = SVR(kernel=\"linear\")\n",
    "    >>> selector = RFECV(estimator, step=1, cv=5)\n",
    "    >>> selector = selector.fit(X, y)\n",
    "    >>> selector.support_\n",
    "    array([ True,  True,  True,  True,  True, False, False, False, False,\n",
    "           False])\n",
    "    >>> selector.ranking_\n",
    "    array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
    "    \"\"\"\n",
    "\n",
    "    _parameter_constraints: dict = {\n",
    "        **RFE._parameter_constraints,\n",
    "        \"min_features_to_select\": [Interval(Integral, 0, None, closed=\"neither\")],\n",
    "        \"cv\": [\"cv_object\"],\n",
    "        \"scoring\": [None, str, callable],\n",
    "        \"n_jobs\": [None, Integral],\n",
    "    }\n",
    "    _parameter_constraints.pop(\"n_features_to_select\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator,\n",
    "        *,\n",
    "        step=1,\n",
    "        min_features_to_select=1,\n",
    "        cv=None,\n",
    "        scoring=None,\n",
    "        verbose=0,\n",
    "        n_jobs=None,\n",
    "        importance_getter=\"auto\",\n",
    "    ):\n",
    "        self.estimator = estimator\n",
    "        self.step = step\n",
    "        self.importance_getter = importance_getter\n",
    "        self.cv = cv\n",
    "        self.scoring = scoring\n",
    "        self.verbose = verbose\n",
    "        self.n_jobs = n_jobs\n",
    "        self.min_features_to_select = min_features_to_select\n",
    "\n",
    "    def fit(self, X, y, groups=None):\n",
    "        \"\"\"Fit the RFE model and automatically tune the number of selected features.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            Training vector, where `n_samples` is the number of samples and\n",
    "            `n_features` is the total number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values (integers for classification, real numbers for\n",
    "            regression).\n",
    "        groups : array-like of shape (n_samples,) or None, default=None\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
    "            instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n",
    "            .. versionadded:: 0.20\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        self._validate_params()\n",
    "        tags = self._get_tags()\n",
    "        X, y = self._validate_data(\n",
    "            X,\n",
    "            y,\n",
    "            accept_sparse=\"csr\",\n",
    "            ensure_min_features=2,\n",
    "            force_all_finite=not tags.get(\"allow_nan\", True),\n",
    "            multi_output=True,\n",
    "        )\n",
    "\n",
    "        # Initialization\n",
    "        cv = check_cv(self.cv, y, classifier=is_classifier(self.estimator))\n",
    "        scorer = check_scoring(self.estimator, scoring=self.scoring)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        if 0.0 < self.step < 1.0:\n",
    "            step = int(max(1, self.step * n_features))\n",
    "        else:\n",
    "            step = int(self.step)\n",
    "\n",
    "        # Build an RFE object, which will evaluate and score each possible\n",
    "        # feature count, down to self.min_features_to_select\n",
    "        rfe = RFE(\n",
    "            estimator=self.estimator,\n",
    "            n_features_to_select=self.min_features_to_select,\n",
    "            importance_getter=self.importance_getter,\n",
    "            step=self.step,\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "\n",
    "        # Determine the number of subsets of features by fitting across\n",
    "        # the train folds and choosing the \"features_to_select\" parameter\n",
    "        # that gives the least averaged error across all folds.\n",
    "\n",
    "        # Note that joblib raises a non-picklable error for bound methods\n",
    "        # even if n_jobs is set to 1 with the default multiprocessing\n",
    "        # backend.\n",
    "        # This branching is done so that to\n",
    "        # make sure that user code that sets n_jobs to 1\n",
    "        # and provides bound methods as scorers is not broken with the\n",
    "        # addition of n_jobs parameter in version 0.18.\n",
    "\n",
    "        if effective_n_jobs(self.n_jobs) == 1:\n",
    "            parallel, func = list, _rfe_single_fit\n",
    "        else:\n",
    "            parallel = Parallel(n_jobs=self.n_jobs)\n",
    "            func = delayed(_rfe_single_fit)\n",
    "\n",
    "        scores = parallel(\n",
    "            func(rfe, self.estimator, X, y, train, test, scorer)\n",
    "            for train, test in cv.split(X, y, groups)\n",
    "        )\n",
    "\n",
    "        scores = np.array(scores)\n",
    "        scores_sum = np.sum(scores, axis=0)\n",
    "        scores_sum_rev = scores_sum[::-1]\n",
    "        argmax_idx = len(scores_sum) - np.argmax(scores_sum_rev) - 1\n",
    "        n_features_to_select = max(\n",
    "            n_features - (argmax_idx * step), self.min_features_to_select\n",
    "        )\n",
    "\n",
    "        # Re-execute an elimination with best_k over the whole set\n",
    "        rfe = RFE(\n",
    "            estimator=self.estimator,\n",
    "            n_features_to_select=n_features_to_select,\n",
    "            step=self.step,\n",
    "            importance_getter=self.importance_getter,\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "\n",
    "        rfe.fit(X, y)\n",
    "\n",
    "        # Set final attributes\n",
    "        self.support_ = rfe.support_\n",
    "        self.n_features_ = rfe.n_features_\n",
    "        self.ranking_ = rfe.ranking_\n",
    "        self.estimator_ = clone(self.estimator)\n",
    "        self.estimator_.fit(self._transform(X), y)\n",
    "\n",
    "        # reverse to stay consistent with before\n",
    "        scores_rev = scores[:, ::-1]\n",
    "        self.cv_results_ = {}\n",
    "        self.cv_results_[\"mean_test_score\"] = np.mean(scores_rev, axis=0)\n",
    "        self.cv_results_[\"std_test_score\"] = np.std(scores_rev, axis=0)\n",
    "\n",
    "        for i in range(scores.shape[0]):\n",
    "            self.cv_results_[f\"split{i}_test_score\"] = scores_rev[i]\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elimination\n",
      "Remaining features: while np.sum(support_) > n_features_to_select [0 1 2 3 4 5 6 7]\n",
      "Get importance and rank them: ranks [2 0 6 4 3 7 5 1]\n",
      "threshold = min(step, np.sum(support_) - n_features_to_select 1\n",
      "Remaining features: while np.sum(support_) > n_features_to_select [0 1 3 4 5 6 7]\n",
      "Get importance and rank them: ranks [0 5 2 3 6 4 1]\n",
      "threshold = min(step, np.sum(support_) - n_features_to_select 1\n",
      "Remaining features: while np.sum(support_) > n_features_to_select [1 3 4 5 6 7]\n",
      "Get importance and rank them: ranks [4 1 5 2 3 0]\n",
      "threshold = min(step, np.sum(support_) - n_features_to_select 1\n",
      "Remaining features: while np.sum(support_) > n_features_to_select [1 3 4 5 7]\n",
      "Get importance and rank them: ranks [1 2 3 4 0]\n",
      "threshold = min(step, np.sum(support_) - n_features_to_select 1\n",
      "Remaining features: while np.sum(support_) > n_features_to_select [1 4 5 7]\n",
      "Get importance and rank them: ranks [1 3 2 0]\n",
      "threshold = min(step, np.sum(support_) - n_features_to_select 1\n",
      "Remaining features: while np.sum(support_) > n_features_to_select [1 5 7]\n",
      "Get importance and rank them: ranks [2 1 0]\n",
      "threshold = min(step, np.sum(support_) - n_features_to_select 1\n",
      "Remaining features: while np.sum(support_) > n_features_to_select [1 5]\n",
      "Get importance and rank them: ranks [1 0]\n",
      "threshold = min(step, np.sum(support_) - n_features_to_select 1\n",
      "ranking_ [7 1 8 5 4 2 6 3]\n",
      "Set final attributes [1]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [47], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m rfe \u001b[39m=\u001b[39m RFE(estimator\u001b[39m=\u001b[39mxgbc, n_features_to_select\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m#### changehere\u001b[39;00m\n\u001b[0;32m      9\u001b[0m rfe\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m---> 11\u001b[0m rfe_XGBoostfeatures \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mFeature\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mlist\u001b[39m(X_\u001b[39m.\u001b[39mcolumns),\n\u001b[0;32m     12\u001b[0m                             \u001b[39m'\u001b[39m\u001b[39mvalue_rfe\u001b[39m\u001b[39m'\u001b[39m:rfe\u001b[39m.\u001b[39mranking_})\n\u001b[0;32m     14\u001b[0m \u001b[39m# rfe_XGBoostfeatures['value_XGBooste'] = rfe_XGBoostfeatures['value_XGBooste'].astype(int)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m# rfe_XGBoostfeatures['value_XGBooste'] = rfe_XGBoostfeatures['value_XGBooste'].map('{:,.19f}'.format)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m\"\"\"The RFE assigns a ranking value to each feature, typically starting from 1 for \u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mthe most important feature, and increasing for the less important features.\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_' is not defined"
     ]
    }
   ],
   "source": [
    "# Recursive Feature Elimination(RFE): Wrapper method\n",
    "\n",
    "\n",
    "\"\"\"In this example, the parameter \"n_features_to_select\" is set to 1, \n",
    "which means that only one feature will be selected at each iteration \n",
    "of RFE\"\"\"\n",
    "\n",
    "rfe = RFE(estimator=xgbc, n_features_to_select=1) #### changehere\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "rfe_XGBoostfeatures = pd.DataFrame({'Feature':list(X_.columns),\n",
    "                            'value_rfe':rfe.ranking_})\n",
    "\n",
    "# rfe_XGBoostfeatures['value_XGBooste'] = rfe_XGBoostfeatures['value_XGBooste'].astype(int)\n",
    "# rfe_XGBoostfeatures['value_XGBooste'] = rfe_XGBoostfeatures['value_XGBooste'].map('{:,.19f}'.format)\n",
    "\"\"\"The RFE assigns a ranking value to each feature, typically starting from 1 for \n",
    "the most important feature, and increasing for the less important features.\"\"\"\n",
    "rfe_XGBoostfeatures = rfe_XGBoostfeatures.sort_values(by=['value_rfe'],ascending=True)\n",
    "rfe_XGBoostfeatures = rfe_XGBoostfeatures.set_index('Feature')\n",
    "\n",
    "# rfe_XGBoostfeatures.to_csv('rfe_XGBoostfeatures.csv', index=True)\n",
    "\n",
    "rfe_XGBoostfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value_rfe</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Glucose</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Insulin</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkinThickness</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pregnancies</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BloodPressure</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          value_rfe\n",
       "Feature                            \n",
       "Glucose                           1\n",
       "BMI                               2\n",
       "Age                               3\n",
       "Insulin                           4\n",
       "SkinThickness                     5\n",
       "DiabetesPedigreeFunction          6\n",
       "Pregnancies                       7\n",
       "BloodPressure                     8"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe_XGBoostfeatures = pd.DataFrame({'Feature':list(X_.columns),\n",
    "                            'value_rfe':rfe.ranking_})\n",
    "\n",
    "# rfe_XGBoostfeatures['value_XGBooste'] = rfe_XGBoostfeatures['value_XGBooste'].astype(int)\n",
    "# rfe_XGBoostfeatures['value_XGBooste'] = rfe_XGBoostfeatures['value_XGBooste'].map('{:,.19f}'.format)\n",
    "\"\"\"The RFE assigns a ranking value to each feature, typically starting from 1 for \n",
    "the most important feature, and increasing for the less important features.\"\"\"\n",
    "rfe_XGBoostfeatures = rfe_XGBoostfeatures.sort_values(by=['value_rfe'],ascending=True)\n",
    "rfe_XGBoostfeatures = rfe_XGBoostfeatures.set_index('Feature')\n",
    "\n",
    "# rfe_XGBoostfeatures.to_csv('rfe_XGBoostfeatures.csv', index=True)\n",
    "\n",
    "rfe_XGBoostfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7b8a86de69880339ca74a4935c0a88da119288d01f69fc6866c17e69e18d7af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
